{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat to your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat to your data! Select a document and chat to GPT about its content. Prompt it to summarize the text, provide details on a specific point or explain understood concepts. \n",
    "\n",
    "Exploiting both langchain and openai libraries (GPT3.5, GPT4), you can load data from a wide range of sources (pdf, doc, spreadsheet, url, audio). In this demo, we show how to work with pdf, a website and youtube link. \n",
    "\n",
    "We will show how Langchain framework simplifies data loading and manipulation and interaction with openAI. First, we split large documents into chunks that can be easily handled by (Large Language Models) LLMs, import embeddings and create vector database stores from these. Finally, we create a chain, with a memory feature, that keeps track of the entire conversation. \n",
    "\n",
    "This notebook has been inspired by the DeepLearning.AI course [\"LangChain: Chat with Your Data\"](https://www.deeplearning.ai/short-courses/). For more details check: \n",
    "- [OpenAI docs](https://platform.openai.com/docs/introduction) \n",
    "- [LangChain](https://python.langchain.com/docs/get_started/introduction.html)\n",
    "\n",
    "Access to GPT requires to create an account in openAI and get and API key. This can be done at [openAI](https://platform.openai.com/account/api-keys). Fees are very low. For instance, if you want to do several tests for a month, loading tens documents, it may be less than 1 $. You can also set a limit to your expenses and track the usage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*LangChain* is a framework that facilitates the development of applications using LLMs. It comprises \n",
    "- **components**, modular abstractions to easily interact with LLM libraries, and\n",
    "- **off-the-self chains** that assembly several components for specific higher-level tasks. \n",
    "\n",
    "Ready made chains allow to start using only a few lines of codes. Then, components allow for customization. \n",
    "\n",
    "LangChain comprises the following modules: **model I/O** (interface with LM), **data connection** (interface with data sources), **chains** (sequence of calls), **agents** (automatic selection of chains), **memory** (track state between chain calls), and **callbacks** (logging). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required modules and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from getpass import getpass\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API key from OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to open an account in openAI and create a key, following these steps. Provide the organization and API key from your OpenAI account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and set API KEY\n",
    "mode_key_json = True\n",
    "if mode_key_json is True:\n",
    "    \"\"\"\n",
    "    Load key from json file. \n",
    "    openai_key.json:     \n",
    "        {\"organization\": <org_key>, \n",
    "        \"api_key\": <api_key>}\n",
    "    \"\"\"\n",
    "    user = 'abascal'\n",
    "    path_file_key = f'/home/{user}/Projects/openai'\n",
    "    name_file_key = \"openai_key.json\" \n",
    "\n",
    "    def read_key_from_file(path_file, name_file_key):\n",
    "        with open(os.path.join(path_file, name_file_key), 'r') as f:\n",
    "            org_data = json.load(f)\n",
    "            \n",
    "        openai.organization = org_data['organization']\n",
    "        openai.api_key = org_data['api_key']\n",
    "\n",
    "    # Read OpenAI key from filepath_file\n",
    "    openai_key = read_key_from_file(path_file_key, name_file_key)\n",
    "else: \n",
    "    # Provide key on the notebook\n",
    "    if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "        if any(['VSCODE' in x for x in os.environ.keys()]):\n",
    "            print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
    "        os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
    "        openai.api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "    assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
    "    print(\"OpenAI API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API key from wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W&W is a platform that allows to build faster and better AI models by tracking experiments, versioning and iterating on datasets, faccilitating model evaluation and reproducibility, and sharing results on data dashboard. \n",
    "\n",
    "For more details on MLOps (Machine Learning Operations) -- a set of practices for collaboration and communication between data scientists and operations professionals -- check [MLOps](https://neptune.ai/blog/mlops) and [best experiment tracking tools](https://neptune.ai/blog/best-ml-experiment-tracking-tools). \n",
    "\n",
    "You can open a free account and create a key, following the steps provided at [W&B](https://docs.wandb.ai/quickstart). \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_wandb = True\n",
    "if mode_wandb:\n",
    "    import wandb\n",
    "    # Automated logging W&B with Langchain\n",
    "    os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
    "    project_name = \"chat-to-your-data\"\n",
    "    os.environ[\"WANDB_PROJECT\"] = project_name\n",
    "    if False:\n",
    "        from wandb.integration.openai import autolog\n",
    "        autolog({\"project\":\"llmapps\", \"job_type\": \"introduction\"})\n",
    "\n",
    "    # Log in (the first time)\n",
    "    # Alternatively run on cmd 'wandb login'\n",
    "    # wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select document type and path to the document. We provide the following examples:\n",
    "- url: URL of lemonde news or wikipedia\n",
    "- pdf: Path to a pdf document\n",
    "- youtube: Link to youtube song or news. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_type = \"url\"                # Document type: \"pdf\" or \"url\" or \"youtube\"\n",
    "\n",
    "if example_type == \"url\":\n",
    "    doc_type = \"url\" # \"pdf\" or \"url\" or \"youtube\"\n",
    "    doc_path = \"https://en.wikipedia.org/wiki/Cinque_Terre\"\n",
    "elif example_type == \"pdf\":\n",
    "    doc_type = \"pdf\" \n",
    "    doc_path = \"../data/Tan_EfficientDetScalableEfficientObjectDetection20.pdf\"\n",
    "elif example_type == \"youtube\":\n",
    "    doc_type = \"youtube\" \n",
    "    #doc_path = \"https://www.youtube.com/watch?v=PNVgh4ZSjCw\"\n",
    "    doc_path = \"https://www.youtube.com/watch?v=W0DM5lcj6mw\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data and transcribe to text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain provides **document loaders** (load different data sources), **document transformers** (split, convert and select documents), **text embedding models** (map unstructure text into a measurement space with a distance metric), **vector stores** (store data and search over embedded data), and **retrievers** (query your data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load data from a given source into a *Document*, which is a piece of text with associated metadata. Examples of document loaders from the module [*langchain.document_loaders*](https://python.langchain.com/docs/modules/data_connection/document_loaders/) are *TextLoader*, *CSVLoader*, *DirectoryLoader*, *PythonLoader*, *UnstructureHTMLLoader*, *BSHTMLLoader*, *JSONLoader*, *UnstructuredMarkdownLoader*, *PDFLoader*, *MathpixPDFLoader*, *UnstructuredPDFLoader*, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear white lines in web pages\n",
    "def clear_blank_lines(docs):\n",
    "    for doc in docs:\n",
    "        doc.page_content = re.sub(r\"\\n\\n\\n+\", \"\\n\\n\", doc.page_content)\n",
    "    return docs\n",
    "\n",
    "# Read document with langchain.document_loaders\n",
    "def read_doc(doc_type, doc_path):\n",
    "    if doc_type == \"pdf\":\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        loader = PyPDFLoader(doc_path)\n",
    "        docs = loader.load()\n",
    "    elif doc_type == \"url\":\n",
    "        from langchain.document_loaders import WebBaseLoader\n",
    "        url = doc_path\n",
    "        loader = WebBaseLoader(url)\n",
    "        docs = loader.load()\n",
    "    elif doc_type == \"youtube\":\n",
    "        from langchain.document_loaders.blob_loaders.youtube_audio import \\\n",
    "            YoutubeAudioLoader\n",
    "        from langchain.document_loaders.generic import GenericLoader\n",
    "        from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
    "        save_path = \"./downloads\"\n",
    "        url = doc_path\n",
    "        loader = GenericLoader(YoutubeAudioLoader([url], save_path), OpenAIWhisperParser())\n",
    "        docs = loader.load()\n",
    "\n",
    "    # Clear white lines in web pages\n",
    "    clear_blank_lines(docs)\n",
    "\n",
    "    print(f\"Loaded {len(docs)} pages/documents\")\n",
    "    print(f\"First page: {docs[0].metadata}\")\n",
    "    print(docs[0].page_content[:500])\n",
    "    return docs\n",
    "\n",
    "def pretty_print_docs(docs, question = None):\n",
    "    print(f\"\\n{'-' * 100}\\n\")\n",
    "    if question:\n",
    "        print(f\"Question: {question}\")\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"Document {i+1}:\\n\\nMetadata: {doc.metadata}\\n\")\n",
    "        print(doc.page_content)\n",
    "    print(\"\\n\")        \n",
    "\n",
    "# Read document with langchain.document_loaders\n",
    "docs = read_doc(doc_type, doc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading data, these must be split into smaller chunks that fit into the LLM calls. Ideally, chunks are split keeping semantically related pieces together and with some overlap. Several test splitters can be use from the module [*langchain.text_splitter*](https://docs.langchain.com/docs/components/indexing/text-splitters): \n",
    "- *CharacterTextSplitter*: Most basic one based on *\"\\n\\n\"*.\n",
    "- *RecursiveCharacterTextSplitter*: Recommended splitter for generic text. It tries to split into blocks based on the following order *[\"\\n\\n\", \"\\n\", \" \", \"\"]*. You can provide *chunk_size*, *chunk_overlap*, and *add_start_index* (chuck position in the original document). \n",
    "- *MarkdownHeaderTextSplitter*: Split by specifying headers. \n",
    "- *TokenTextSplitter*: Split by tokens, providing the maximum number of tokens, for several tokenizers as *tiktoken* from openAI, *spaCy*, *NLTK*.\n",
    "- *SentenceTransformersTokenTextSplitter*: takes into account the model used, as  hugging face or NLTK tokenizer. \n",
    "\n",
    "Specific texts can benefit from specialized splitters, such as for code (python, java, ...), LaTex, or markdown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Parameters for splitting documents into chunks\n",
    "chunk_size = 1500                   \n",
    "chunk_overlap = 150\n",
    "add_start_index = True\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, \n",
    "    chunk_overlap=chunk_overlap,\n",
    "    add_start_index=add_start_index)\n",
    "\n",
    "docs_split = text_splitter.split_documents(docs)\n",
    "print(f\"Split into {len(docs_split)} chunks\")\n",
    "print(f\"First chunk: {docs_split[0].metadata}\")\n",
    "print(docs_split[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other transformations allow to filter documents, selecting those that are related to provided query, such as *EmbeddingsRedundantFilter*. Re-order documents may be needed for more than 10 documents to avoid performance degradation (see *get_relevant_documents*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to store unstructured data by projecting in into an **embedding vector space**. Then, for prediction, the query is also embedded and the 'closest' embedding vectors are retrieved. A **vector store** is responsible for storing and retrieving embedded data. \n",
    "\n",
    "Possible vector spaces are [*Chroma*](https://python.langchain.com/docs/integrations/vectorstores/chroma) and [*OpenAIEmbeddings*](https://js.langchain.com/docs/api/embeddings_openai/classes/OpenAIEmbeddings). LangChain uses Chroma as the vectorstore to index and search embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "#from langchain.vectorstores import Chroma\n",
    "\n",
    "# Define embedding\n",
    "embedding = OpenAIEmbeddings(openai_api_key=openai.api_key)    \n",
    "\n",
    "# Create vector database from data    \n",
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs_split, \n",
    "    embedding=embedding)\n",
    "\n",
    "#db = Chroma.from_texts(docs_split, embedding=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering documents can be done in the embedded space. For instance, provided a query, we can perform a similarity search to retrieve a given number of documents, using *similarity_search* or *similarity_search_by_vector*. Another possibility that may work better is to compressed documents using *ContextualCompressionRetriever*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_similarity = False\n",
    "if mode_similarity:\n",
    "    k = 5   # Number of documents to return\n",
    "    query = \"Documents that provide details about the artists.\"\n",
    "    # filter = {\"source\": \"file.pdf\"}\n",
    "    db.similarity_search(query, k=k)\n",
    "    # db.max_marginal_relevance_search(query, k=k)\n",
    "\n",
    "    # Or select docs by embedded vector\n",
    "    # embedding_vector = embedding.embed_query(query)\n",
    "    # docs = db.similarity_search_by_vector(embedding_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector store and similarity search can be created also asynchronously using *Qdrant*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM and retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select the LLM as *gpt-3.5-turbo* with *temperature=0*. The lower the temperature the more deterministic the ouput, the higher its value the more random the result ($temperature\\in[0,1]$). Select values lower than 0.3 for text summarization or grammar correction and high values for text generation. You may choose a different one. Then, we initialize the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "#from langchain.llms import OpenAI\n",
    "\n",
    "# Info user API key\n",
    "llm_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "# Init the LLM and memory\n",
    "# llm = OpenAI(temperature=0, openai_api_key=openai_key)\n",
    "llm = ChatOpenAI(model_name=llm_name,\n",
    "                 temperature=0,\n",
    "                 openai_api_key=openai.api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrievers are interfaces that return documents provided a query. However, they do not store documents as vectors stores do. Although there are many retrievers, we focus on those built from vector stores. \n",
    "\n",
    "Retrieval may be inconsistent depending on the query and its embedding. There are several retrievers: *SelfQueryRetriever*, *MultiQueryRetriever*, *ContextualCompressionRetriever*, among others. Below, we show an example of *ContextualCompressionRetriever*, which allows selecting documents by compression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select documents using compression\n",
    "mode_compression = False \n",
    "if mode_compression:\n",
    "    from langchain.retrievers import ContextualCompressionRetriever\n",
    "    from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "    # Wrap our vectorstore\n",
    "    \n",
    "    compressor = LLMChainExtractor.from_llm(llm)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=db.as_retriever()\n",
    "    )\n",
    "    query = \"Documents that provide details about the artists.\"\n",
    "    compressed_docs = compression_retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Chains](https://python.langchain.com/docs/modules/chains/) allow to combine components in order to provide higher-level specific applications. Chat required chains to be initialized with a memory object, which allows persist data during multiple calls. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question answering involves creating an index, creating a retriever from this index, creating a question answering chain and making questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# QA CHAIN\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start interaction\n",
    "qa_on = False # Ask questions to the user\n",
    "while qa_on == True:\n",
    "    # Prompt the user to introduce a question\n",
    "    question = input(\"Ask a question or type 'end chat': \")\n",
    "    \n",
    "    if question.lower() == \"end chat\":\n",
    "        break\n",
    "\n",
    "    # Run QA chain\n",
    "    result = qa_chain({\"question\": question})\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    # -------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a chat app with Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradio is the fastest way to build interfaces for your machine learning  models. Build interactive apps to test your demos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build first a simple interface using [*gr.Interface*](https://www.gradio.app/docs/interface) that wraps any Python function with an interface. In this case we wrap the text function *qa_call* that takes a text as input and return a text as output, or *qa_call_output_history* that returns a string that contains all history. You can customize the text fields and provide a description and examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_call(input):\n",
    "    # QA call\n",
    "    output = qa_chain({\"question\": input})\n",
    "    return output\n",
    "\n",
    "def qa_answer(input):\n",
    "    # Return the answer from the QA call\n",
    "    return qa_call(input)['answer']\n",
    "\n",
    "def qa_history(input):\n",
    "    # Return a formatted history\n",
    "    response = qa_chain({\"question\": input})\n",
    "    output = \"\"\n",
    "    response_history = response['chat_history']\n",
    "    num_qa = len(response_history)//2\n",
    "    for i in range(num_qa):\n",
    "        output += \"Q: \" + response_history[2*i].content + \"\\n\"\n",
    "        output += \"A: \" + response_history[2*i+1].content + \"\\n\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build an interface, providing the inputs, the outputs and the function *qa_call*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = gr.Interface(fn=qa_history, \n",
    "                    inputs=[gr.Textbox(label=\"User question\", \n",
    "                                       lines=2)],\n",
    "                    outputs=[gr.Textbox(label=\"Chat answer\", \n",
    "                                        lines=4)],\n",
    "                    title=\"Chat to your data\",\n",
    "                    description=f\"Ask questions about your data to {llm_name}!\",\n",
    "                    allow_flagging=\"never\",\n",
    "                    examples=[\"Summarize the document\", \"Can you provide details about ...\", \"Can you exaplin what is ...?\"]\n",
    "                   )\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An new alternative is to use *ChatInterface* that provides a minimalistic interface for chatbot UIs, for which only the function parameter is required. This should has the prompt as input and the answer as output, where the history is managed internally. Other parameters are allowed for customizing the interface: [chatinterface](https://www.gradio.app/docs/chatinterface). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_input_msg_history(input, history):\n",
    "    # QA function that inputs the answer and the history\n",
    "    # History managed internally by ChatInterface\n",
    "    answer = qa_answer(input)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init memory and QA chain\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "demo = gr.ChatInterface(fn=qa_input_msg_history, \n",
    "                    title=\"Chat to your data\",\n",
    "                    description=f\"Ask questions about your data to {llm_name}!\",\n",
    "                    examples=[\"Summarize the document\", \"Can you provide details about ...\", \"Can you exaplin what is ...?\"]\n",
    "                   )\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode_wandb:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
