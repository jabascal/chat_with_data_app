{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From chatting to talking to any source of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converse to any source of data while doing any activity (walking on the treadmill, cooking, cleaning, ...) - a tutorial on leveraging OpenAI's GPT and Whisper models and Python libraries for audio processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous tutorial ([Chat to any source of data](https://medium.com/@juanabascal78/chat-to-any-source-of-data-with-langchain-and-openai-3677ecb8665d)), we show how to exploit *Langchain* and *OpenAI* to chat to any data (pdf, url, youtube link, xlsx, tex, ...) . In this tutorial, we level up and show how to transcribe text to audio and vice versa while interacting with both GPT and Whisper. Make your APIs talk as the now talking [ChatGPT](https://openai.com/blog/chatgpt-can-now-see-hear-and-speak)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we install the required dependencies in the environment of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraires\n",
    "!python -m venv venv\n",
    "!cd venv\n",
    "!source venv/bin/activate\n",
    "!pip install -r requirements_talk_to_your_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the requirements.txt file is given below\n",
    "```\n",
    "openai==1.2.4\n",
    "langchain==0.0.335\n",
    "chromadb==0.3.26\n",
    "pydantic==1.10.8\n",
    "langchain[docarray]\n",
    "gTTS==2.4.0\n",
    "pvrecorder==1.2.1\n",
    "playsound==1.3.0\n",
    "bs4==0.0.1\n",
    "tiktoken==0.5.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API Key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need an OpenAI API key, which we will read from a JSON file. It may also use an environment variable ([best practices](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "\n",
    "path_file_key = '/home/username/openai'\n",
    "name_file_key = \"openai_key.json\" \n",
    "    \n",
    "\"\"\"\n",
    "json file: \n",
    "openai_key.json:     \n",
    "    {\"organization\": <org_key>, \n",
    "    \"api_key\": <api_key>}\n",
    "\"\"\"\n",
    "\n",
    "def read_key_from_file(path_file, name_file_key):\n",
    "    with open(os.path.join(path_file, name_file_key), 'r') as f:\n",
    "        org_data = json.load(f)\n",
    "        \n",
    "    openai.organization = org_data['organization']\n",
    "    openai.api_key = org_data['api_key']\n",
    "\n",
    "# Read OpenAI key from filepath_file\n",
    "openai_key = read_key_from_file(path_file_key, name_file_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we are all set, letâ€™s quick off with the fun stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe text to audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore the functionality required to transcribe text to speech and vice versa. The easiest part is to transcribe text to natural sound. For this, we would compare two different methods: \n",
    "- gTTS ([Google Text-to-Speech](https://gtts.readthedocs.io/en/latest/index.html)), which leverages Google Translate speach functionality, providing  text-to-speech transcription that allows unlimited lengths of text, keeping proper intonation, and abbreviations. It supports several languages and accents ([gTTS accents](https://gtts.readthedocs.io/en/latest/module.html)). To see the available languages: `gtts-cli --all`. For instance, we can use the following commands:\n",
    "```\n",
    "    gTTS('hello', lang='en', tld='co.uk')\n",
    "    gTTS('bonjour', lang='fr')\n",
    "```\n",
    "- OpenAI's text-to-speech model [tts-1](https://platform.openai.com/docs/guides/text-to-speech). It allows for different voice options (`alloy`, `echo`, `fable`, ...) and supports a wide range of languages a(same as Whisper model). It also supports for real time audio streaming using chunk transfer encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we transcribe text to speech and then write it to file with `gTTS`. To play audio we use the python's library [playsound](https://pypi.org/project/playsound/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "from playsound import playsound\n",
    "\n",
    "def play_text(text, language='en', accent='co.uk', file_audio=\"../tmp/audio.wav\"):\n",
    "    \"\"\" \n",
    "    play_text: Play text with gTTS and playsound libraries. It writes the audio file\n",
    "    first and then plays it.\n",
    "    \"\"\"\n",
    "    gtts = gTTS(text, lang=language)\n",
    "    gtts.save(file_audio)\n",
    "    playsound(file_audio)\n",
    "\n",
    "text = \"Hello, how are you Today? It's a beataful day, isn't it? Have a nice day!\"\n",
    "play_text(text, file_audio=\"../tmp/hello.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare to OpenAI's TTS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to speech with openAI\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "def play_text_oai(text, file_audio=\"../tmp/audio.mp3\", model=\"tts-1\", voice=\"alloy\"):\n",
    "    \"\"\" \n",
    "    play_text_oai: Play text with OpeanAI and playsound libraries. It writes the audio file\n",
    "    first and then plays it.\n",
    "    \"\"\"\n",
    "    speech_file_path = \"../tmp/hello.mp3\"\n",
    "    response = openai.audio.speech.create(\n",
    "      model=model,\n",
    "      voice=voice,\n",
    "      input=text\n",
    "    )\n",
    "    response.stream_to_file(file_audio)\n",
    "    playsound(file_audio)\n",
    "\n",
    "play_text_oai(text, file_audio=\"../tmp/audio.mp3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI's model provides a more natural language and corrects the spelling mistake! However, if you provide a foreign address, it will be wrongly transcribed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe audio to text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key part is to **transcribe audio to text**. For this we use [openai.audio.transcribe](https://platform.openai.com/docs/guides/speech-to-text), which provides speech-to-text transcriptions for many languages and translation to English based on OpenAI \"Whisper\" model. It supports several audio formats (mp3, mp4, mav and others), with a limit of 25 MB, and text formats (json default). \n",
    "\n",
    "Whisper is an automatically speech recognition (ASR) system, trained on 680,000 hours of multilingual (98 languages) and multitask supervised data collected from the web. Trained on a large dataset, it may be less accurate than other models trained in specific datasets but should be more robust to new data. It also beats many translation models. It is based on a encoder-decoder transformer architecture. Audio is split into 30s chunks, converted to log-Mel spectogram, and trained to predict the next token on several tasks (language identification, transcription, and to-English speech translation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to record audio and write it to file. For recording audio, we use [PvRecorder](https://pypi.org/project/pvrecorder/), an easy-to-use, cross platform audio recorder designed for real-time speech audio processing. For writing audio to file, we use [wave](https://docs.python.org/3/library/wave.html), which allows to easily read and write WAV files. Other options are [soundfile](https://pypi.org/project/SoundFile/) and [pydub](https://pypi.org/project/pydub/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pvrecorder import PvRecorder\n",
    "devices = PvRecorder.get_available_devices()\n",
    "print(devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import struct\n",
    "from pvrecorder import PvRecorder\n",
    "\n",
    "def write_audio_to_file(audio, \n",
    "                        audio_frequency=16000, \n",
    "                        file_audio=\"tmp.wav\"):\n",
    "    \"\"\" \n",
    "    write_audio_to_file: Write audio to file with wave library.\n",
    "    \"\"\"\n",
    "    with wave.open(file_audio, 'w') as f:\n",
    "        f.setparams((1, 2, audio_frequency, len(audio), \"NONE\", \"NONE\"))\n",
    "        f.writeframes(struct.pack(\"h\" * len(audio), *audio))\n",
    "\n",
    "\n",
    "def record_audio(device_index=-1, \n",
    "                 frame_length=512, # audio samples at each read\n",
    "                 num_frames = 600, # 20 seconds\n",
    "                 audio_frequency=16000, \n",
    "                 file_audio=\"tmp.wav\"):\n",
    "    \"\"\"\n",
    "    record_audio: Record audio with pvrecorder library.\n",
    "    \"\"\"\n",
    "\n",
    "    # Record audio\n",
    "    # Init the recorder\n",
    "    recorder = PvRecorder(frame_length=frame_length, device_index=device_index)\n",
    "\n",
    "    print(\"\\nRecording...\")\n",
    "    try:\n",
    "        audio = []\n",
    "        recorder.start()\n",
    "        for fr_id in range(num_frames):\n",
    "            frame = recorder.read()\n",
    "            audio.extend(frame)\n",
    "        write_audio_to_file(audio, audio_frequency=audio_frequency, file_audio=file_audio)\n",
    "        recorder.stop()\n",
    "    except KeyboardInterrupt:\n",
    "        recorder.stop()\n",
    "        write_audio_to_file(audio, audio_frequency=audio_frequency, file_audio=file_audio)\n",
    "    finally:\n",
    "        recorder.delete()\n",
    "    print(\"Recording finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we text the audio recording. Run the following code to record and play back the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record audio sample and play it\n",
    "play_text_oai(\"Please, say something (you have 5 seconds)\", file_audio=\"../tmp/tmp.wav\")\n",
    "record_audio(file_audio=\"../tmp/audio.wav\", num_frames=150, device_index=-1)\n",
    "play_text_oai(\"You said\", file_audio=\"../tmp/tmp.wav\")\n",
    "playsound(\"../tmp/audio.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribe audio file to text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to transcribe audio to text using `openai.audio.transcriptions`. For this, we set the LLM name to `whisper-1` and specify the user language. The language is key to get good results; otherwise, it may get confuse with accents. \n",
    "\n",
    "To test it, we record some audio, play it back and print the transcribed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM name\n",
    "llm_audio_name = \"whisper-1\"\n",
    "\n",
    "# Language of user speech (For better accuracy; otherwise accents lead to errors)\n",
    "language_user = \"en\"                \n",
    "\n",
    "# Record audio sample and play it\n",
    "play_text_oai(\"Please, say something (you have 10 seconds)\", file_audio=\"../tmp/tmp.wav\")\n",
    "record_audio(file_audio=\"../tmp/audio.wav\", num_frames=300, device_index=-1)\n",
    "play_text_oai(\"Now, we print what you said:\", file_audio=\"../tmp/tmp.wav\")\n",
    "\n",
    "# Read audio file and transcribe it\n",
    "audio_file = open(os.path.join(\"../tmp\", \"audio.wav\"), \"rb\")\n",
    "#transcript = openai.Audio.transcribe(llm_audio_name, \n",
    "#                                        audio_file,\n",
    "#                                        language=language_user)\n",
    "text = openai.audio.transcriptions.create(model=\"whisper-1\", file=audio_file, \n",
    "                                          response_format=\"text\")\n",
    "print(f\"\\nQuestion: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not in real time. That would require chunk streaming and lots of optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap on LangChain and OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the previous tutorial ([Chat to any source of data](https://medium.com/@juanabascal78/chat-to-any-source-of-data-with-langchain-and-openai-3677ecb8665d)) for a short introduction to *Langchain* on data loading, splitting data into chunks, using embeddings, creating vector database stores and creating high-level chains to easily interact with a LLM. We recap the main steps in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define any source of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document type: \"pdf\" or \"url\" or \"youtube\"\n",
    "example_type = \"url\"                \n",
    "\n",
    "if example_type == \"url\":\n",
    "    doc_type = \"url\" \n",
    "    doc_path = \"https://en.wikipedia.org/wiki/Cinque_Terre\"\n",
    "elif example_type == \"pdf\":\n",
    "    doc_type = \"pdf\" \n",
    "    doc_path = \"../data/Adler_DeepPosteriorSampling19.pdf\"\n",
    "elif example_type == \"youtube\":\n",
    "    doc_type = \"youtube\" \n",
    "    #doc_path = \"https://www.youtube.com/watch?v=PNVgh4ZSjCw\"\n",
    "    doc_path = \"https://www.youtube.com/watch?v=W0DM5lcj6mw\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Clear white lines in web pages\n",
    "def clear_blank_lines(docs):\n",
    "    for doc in docs:\n",
    "        doc.page_content = re.sub(r\"\\n\\n\\n+\", \"\\n\\n\", doc.page_content)\n",
    "    return docs\n",
    "\n",
    "# Read document with langchain.document_loaders\n",
    "def read_doc(doc_type, doc_path):\n",
    "    if doc_type == \"pdf\":\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        loader = PyPDFLoader(doc_path)\n",
    "        docs = loader.load()\n",
    "    elif doc_type == \"url\":\n",
    "        from langchain.document_loaders import WebBaseLoader\n",
    "        url = doc_path\n",
    "        loader = WebBaseLoader(url)\n",
    "        docs = loader.load()\n",
    "    elif doc_type == \"youtube\":\n",
    "        # See the requirements file for the extra required libraries\n",
    "        # Not working currently on langchain with current openAI version for STT!!!\n",
    "        from langchain.document_loaders.blob_loaders.youtube_audio import \\\n",
    "            YoutubeAudioLoader\n",
    "        from langchain.document_loaders.generic import GenericLoader\n",
    "        from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
    "        save_path = \"./downloads\"\n",
    "        url = doc_path\n",
    "        loader = GenericLoader(YoutubeAudioLoader([url], save_path), OpenAIWhisperParser())\n",
    "        docs = loader.load()\n",
    "\n",
    "    # Clear white lines in web pages\n",
    "    clear_blank_lines(docs)\n",
    "\n",
    "    print(f\"Loaded {len(docs)} pages/documents\")\n",
    "    print(f\"First page: {docs[0].metadata}\")\n",
    "    print(docs[0].page_content[:500])\n",
    "    return docs\n",
    "\n",
    "def pretty_print_docs(docs, question = None):\n",
    "    print(f\"\\n{'-' * 100}\\n\")\n",
    "    if question:\n",
    "        print(f\"Question: {question}\")\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"Document {i+1}:\\n\\nMetadata: {doc.metadata}\\n\")\n",
    "        print(doc.page_content)\n",
    "    print(\"\\n\")        \n",
    "\n",
    "# Read document with langchain.document_loaders\n",
    "docs = read_doc(doc_type, doc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Parameters for splitting documents into chunks\n",
    "chunk_size = 1500                   \n",
    "chunk_overlap = 150\n",
    "add_start_index = True\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, \n",
    "    chunk_overlap=chunk_overlap,\n",
    "    add_start_index=add_start_index)\n",
    "\n",
    "docs_split = text_splitter.split_documents(docs)\n",
    "print(f\"Split into {len(docs_split)} chunks\")\n",
    "print(f\"First chunk: {docs_split[0].metadata}\")\n",
    "print(docs_split[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the conversational chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "#from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "# Info user API key\n",
    "llm_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "# Init the LLM and memory\n",
    "# llm = OpenAI(temperature=0, openai_api_key=openai_key)\n",
    "llm = ChatOpenAI(model_name=llm_name,\n",
    "                 temperature=0,\n",
    "                 openai_api_key=openai.api_key)\n",
    "\n",
    "# Memory buffer\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Define embedding\n",
    "embedding = OpenAIEmbeddings(openai_api_key=openai.api_key)    \n",
    "\n",
    "# Create vector database from data    \n",
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs_split, \n",
    "    embedding=embedding)\n",
    "\n",
    "# Conversational chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a talking chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we got to the point when we can maintain a conversation with our data. We start by defining some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# LLM name\n",
    "llm_name = \"gpt-4\"\n",
    "llm_audio_name = \"whisper-1\"\n",
    "\n",
    "# Document\n",
    "example_type = \"url\"                # Document type: \"pdf\" or \"url\" or \"youtube\"\n",
    "chunk_size = 1500                   # Parameters for splitting documents into chunks\n",
    "chunk_overlap = 150\n",
    "mode_input = \"file\"                 # Mode read: \"file\" or \"db\", db if db already saved to drive (avoid reading it)\n",
    "\n",
    "# Mode of interaction\n",
    "question_mode = \"audio\"             # \"text\" or \"audio\"\n",
    "language_user = \"en\"                # Language of user speech (For better accuracy; otherwise accents lead to errors)\n",
    "language_answer = \"en\"              # Desired language for reply speech (gTTS)\n",
    "\n",
    "# Parameters for recording audio\n",
    "audio_frequency = 16000\n",
    "frame_length = 512                  # audio samples at each read\n",
    "num_frames = 300                    # 600 for 20 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we specify text scripts to interact with the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tmp = \"../tmp\"                         # Path to save audio\n",
    "name_tmp_audio = \"audio.mp3\"                  \n",
    "file_audio_intro = \"../tmp/talk_intro.mp3\"  # Audio temporal files\n",
    "file_audio_question = \"../tmp/question.mp3\"\n",
    "file_audio_answer = \"../tmp/answer.mp3\"\n",
    "\n",
    "persist_path = \"./docs/chroma\"              # Persist path to save vector database\n",
    " \n",
    "if not os.path.exists(path_tmp):\n",
    "    os.makedirs(path_tmp)\n",
    "file_tmp_audio = os.path.join(path_tmp, name_tmp_audio)\n",
    "\n",
    "# Audio samples\n",
    "text_intro = f\"\"\"\n",
    "You are chatting to {llm_name}, transcriptions by {llm_audio_name}, \n",
    "about the provided {example_type} link. \n",
    "You can ask questions or chat about the document provided, in any language. \n",
    "You have 10 to 20 seconds to make your questions. \n",
    "Answers will be played back to you and printed out in the language selected. \n",
    "To end the chat, say 'End chat' when providing a question.\n",
    "\"\"\"\n",
    "\n",
    "text_question = \"Ask your question\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to chat to our data. We define a function that takes a question and returns an answer. We use the function *chat* from the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start interaction\n",
    "play_text_oai(text_intro, file_audio=file_audio_intro)\n",
    "qa_on = True # Ask questions to the user\n",
    "while qa_on == True:\n",
    "    # Prompt the user to introduce a question\n",
    "    # Play prompt question\n",
    "    print(text_question)\n",
    "    #play_text(text_question, language=language_answer, file_tmp_audio=file_audio_intro)\n",
    "    play_text_oai(text_question, file_audio=file_audio_question)\n",
    "\n",
    "    # Record audio\n",
    "    record_audio(device_index=-1, \n",
    "                    frame_length=frame_length, # audio samples at each read\n",
    "                    num_frames = num_frames, # 20 seconds\n",
    "                    audio_frequency=audio_frequency, \n",
    "                    file_audio=file_tmp_audio)\n",
    "\n",
    "    # Transcribe audio\n",
    "    audio_file = open(file_tmp_audio, \"rb\")\n",
    "    #transcript = openai.Audio.transcribe(llm_audio_name, \n",
    "    #                                        audio_file,\n",
    "    #                                        language=language_user)\n",
    "    question = openai.audio.transcriptions.create(model=\"whisper-1\", file=audio_file, \n",
    "                                          response_format=\"text\", language=language_user)\n",
    "    #question = transcript['text']\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    \n",
    "    if question.lower() == \"End chat\":\n",
    "        break\n",
    "\n",
    "    # -------------------------\n",
    "    # Run QA chain\n",
    "    result = qa_chain({\"question\": question})\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "\n",
    "    # Text to speech\n",
    "    if question_mode == \"audio\":\n",
    "        #play_text(result['answer'], language=language_answer, file_tmp_audio=file_audio_answer)\n",
    "        play_text_oai(result['answer'], file_audio=file_audio_answer)\n",
    "    # -------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other speech and audio tools are the following:\n",
    "- **SpeechRecognition**: \n",
    "- **PyAudio** :access devices and record/play audio\n",
    "- **Librosa**: audio analysis, pitch detection, beat tracking, audio segmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
