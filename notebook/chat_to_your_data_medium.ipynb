{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jabascal/chat_with_data_app/blob/main/notebook/chat_to_your_data_medium.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat to any source of data with LangChain and OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_colab = True\n",
    "if mode_colab is False:\n",
    "    # Local installation\n",
    "    !python -m venv venv\n",
    "    !source venv/bin/activate\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document type: \"pdf\" or \"url\" or \"youtube\"\n",
    "example_type = \"url\"                \n",
    "\n",
    "if example_type == \"url\":\n",
    "    doc_type = \"url\" \n",
    "    doc_path = \"https://en.wikipedia.org/wiki/Cinque_Terre\"\n",
    "elif example_type == \"pdf\":\n",
    "    doc_type = \"pdf\" \n",
    "    doc_path = \"./data/paper.pdf\"\n",
    "elif example_type == \"youtube\":\n",
    "    doc_type = \"youtube\" \n",
    "    #doc_path = \"https://www.youtube.com/watch?v=PNVgh4ZSjCw\"\n",
    "    doc_path = \"https://www.youtube.com/watch?v=W0DM5lcj6mw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Clear white lines in web pages\n",
    "def clear_blank_lines(docs):\n",
    "    for doc in docs:\n",
    "        doc.page_content = re.sub(r\"\\n\\n\\n+\", \"\\n\\n\", doc.page_content)\n",
    "    return docs\n",
    "\n",
    "# Read document with langchain.document_loaders\n",
    "def read_doc(doc_type, doc_path):\n",
    "    if doc_type == \"pdf\":\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        loader = PyPDFLoader(doc_path)\n",
    "        docs = loader.load()\n",
    "    elif doc_type == \"url\":\n",
    "        from langchain.document_loaders import WebBaseLoader\n",
    "        url = doc_path\n",
    "        loader = WebBaseLoader(url)\n",
    "        docs = loader.load()\n",
    "    elif doc_type == \"youtube\":\n",
    "        from langchain.document_loaders.blob_loaders.youtube_audio import \\\n",
    "            YoutubeAudioLoader\n",
    "        from langchain.document_loaders.generic import GenericLoader\n",
    "        from langchain.document_loaders.parsers import OpenAIWhisperParser\n",
    "        save_path = \"./downloads\"\n",
    "        url = doc_path\n",
    "        loader = GenericLoader(YoutubeAudioLoader([url], save_path), OpenAIWhisperParser())\n",
    "        docs = loader.load()\n",
    "\n",
    "    # Clear white lines in web pages\n",
    "    clear_blank_lines(docs)\n",
    "\n",
    "    print(f\"Loaded {len(docs)} pages/documents\")\n",
    "    print(f\"First page: {docs[0].metadata}\")\n",
    "    print(docs[0].page_content[:500])\n",
    "    return docs\n",
    "\n",
    "def pretty_print_docs(docs, question = None):\n",
    "    print(f\"\\n{'-' * 100}\\n\")\n",
    "    if question:\n",
    "        print(f\"Question: {question}\")\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"Document {i+1}:\\n\\nMetadata: {doc.metadata}\\n\")\n",
    "        print(doc.page_content)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Read document with langchain.document_loaders\n",
    "docs = read_doc(doc_type, doc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Parameters for splitting documents into chunks\n",
    "chunk_size = 1500                   \n",
    "chunk_overlap = 150\n",
    "add_start_index = True\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, \n",
    "    chunk_overlap=chunk_overlap,\n",
    "    add_start_index=add_start_index)\n",
    "\n",
    "docs_split = text_splitter.split_documents(docs)\n",
    "print(f\"Split into {len(docs_split)} chunks\")\n",
    "print(f\"First chunk: {docs_split[0].metadata}\")\n",
    "print(docs_split[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "\n",
    "user = 'abascal'\n",
    "path_file_key = f'/home/{user}/Projects/openai'\n",
    "name_file_key = \"openai_key.json\" \n",
    "\n",
    "def read_key_from_file(path_file, name_file_key):\n",
    "    with open(os.path.join(path_file, name_file_key), 'r') as f:\n",
    "        org_data = json.load(f)\n",
    "        \n",
    "    openai.organization = org_data['organization']\n",
    "    openai.api_key = org_data['api_key']\n",
    "\n",
    "# Read OpenAI key from filepath_file\n",
    "openai_key = read_key_from_file(path_file_key, name_file_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "#from langchain.vectorstores import Chroma\n",
    "\n",
    "# Define embedding\n",
    "embedding = OpenAIEmbeddings(openai_api_key=openai.api_key)    \n",
    "\n",
    "# Create vector database from data    \n",
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs_split, \n",
    "    embedding=embedding)\n",
    "\n",
    "#db = Chroma.from_texts(docs_split, embedding=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM and retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "#from langchain.llms import OpenAI\n",
    "\n",
    "# Info user API key\n",
    "llm_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "# Init the LLM and memory\n",
    "# llm = OpenAI(temperature=0, openai_api_key=openai_key)\n",
    "llm = ChatOpenAI(model_name=llm_name,\n",
    "                 temperature=0,\n",
    "                 openai_api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# QA CHAIN\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start interaction\n",
    "qa_on = True # Ask questions to the user\n",
    "while qa_on == True:\n",
    "    # Prompt the user to introduce a question\n",
    "    question = input(\"Ask a question or type 'end chat': \")\n",
    "    \n",
    "    if question.lower() == \"end chat\":\n",
    "        break\n",
    "\n",
    "    # Run QA chain\n",
    "    result = qa_chain({\"question\": question})\n",
    "    print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a chat app with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_call(input):\n",
    "    # QA call\n",
    "    output = qa_chain({\"question\": input})\n",
    "    return output\n",
    "\n",
    "def qa_answer(input):\n",
    "    # Return the answer from the QA call\n",
    "    return qa_call(input)['answer']\n",
    "\n",
    "def qa_history(input):\n",
    "    # Return a formatted history\n",
    "    response = qa_chain({\"question\": input})\n",
    "    output = \"\"\n",
    "    response_history = response['chat_history']\n",
    "    num_qa = len(response_history)//2\n",
    "    for i in range(num_qa):\n",
    "        output += \"Q: \" + response_history[2*i].content + \"\\n\"\n",
    "        output += \"A: \" + response_history[2*i+1].content + \"\\n\"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Interface(fn=qa_history, \n",
    "                    inputs=[gr.Textbox(label=\"User question\", \n",
    "                                       lines=2)],\n",
    "                    outputs=[gr.Textbox(label=\"Chat answer\", \n",
    "                                        lines=4)],\n",
    "                    title=\"Chat to your data\",\n",
    "                    description=f\"Ask questions about your data to {llm_name}!\",\n",
    "                    allow_flagging=\"never\",\n",
    "                    examples=[\"Summarize the document\", \"Can you provide details about ...\", \"Can you exaplin what is ...?\"]\n",
    "                   )\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same with ChatInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_input_msg_history(input, history):\n",
    "    # QA function that inputs the answer and the history\n",
    "    # History managed internally by ChatInterface\n",
    "    answer = qa_answer(input)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init memory and QA chain\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "demo = gr.ChatInterface(fn=qa_input_msg_history, \n",
    "                    title=\"Chat to your data\",\n",
    "                    description=f\"Ask questions about your data to {llm_name}!\",\n",
    "                    examples=[\"Summarize the document\", \"Can you provide details about ...\", \"Can you exaplin what is ...?\"]\n",
    "                   )\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
